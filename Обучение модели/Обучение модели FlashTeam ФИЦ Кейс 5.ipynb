{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Код ниже предназначен для запуска в kagle с учетом зависимостей, которые там предустановлены. Для запуска в локальном Юпитер-ноутбуке нужно предварительно установить зависимости из requirements.txt в корне проекта**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Предварительно надо загрузить датасет train_dataset.json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import joblib\n",
    "\n",
    "# Загрузка данных\n",
    "file_path = '/kaggle/input/train-dataset-fic-flashteam/train_dataset.json'  # Сюда вставить путь до датасета\n",
    "data = pd.read_json(file_path)\n",
    "\n",
    "# Подготовка данных\n",
    "columns_to_keep = ['age', 'semantic_similarity', 'skill_similarity', 'total_months_worked', 'grade_proof']\n",
    "data = data[columns_to_keep]\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['grade_proof'] = le.fit_transform(data['grade_proof'])  # \"подтверждён\" -> 1, \"не подтверждён\" -> 0\n",
    "\n",
    "X = data.drop(columns=['grade_proof'])\n",
    "y = data['grade_proof']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Оценка важности признаков с помощью Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "feature_importance = rf_model.feature_importances_\n",
    "\n",
    "# Корректируем веса для VotingClassifier\n",
    "weights = [1, 3, 1]  # Базовые веса\n",
    "semantic_skill_weight = feature_importance[1] + feature_importance[2]\n",
    "age_months_weight = feature_importance[0] + feature_importance[3]\n",
    "\n",
    "weights[1] += int(age_months_weight * 5)  # Увеличиваем вес Random Forest\n",
    "weights[2] += int(semantic_skill_weight * 5)  # Увеличиваем вес XGBoost\n",
    "\n",
    "# Определяем модели\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Random Forest\": rf_model,\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "calibrated_models = {name: CalibratedClassifierCV(model, cv=5) for name, model in models.items()}\n",
    "for name, model in calibrated_models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "# VotingClassifier с обновленными весами\n",
    "voting_ensemble = VotingClassifier(\n",
    "    estimators=[(name, model) for name, model in calibrated_models.items()],\n",
    "    voting='soft',\n",
    "    weights=weights\n",
    ")\n",
    "voting_ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Сохранение обученной модели VotingClassifier\n",
    "joblib.dump(voting_ensemble, \"voting_ensemble_model.pkl\")\n",
    "print(\"Сохранено.\")\n",
    "\n",
    "# Сохранение масштабировщика\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "print(\"Сохранено.\")\n",
    "\n",
    "# Сохранение LabelEncoder\n",
    "joblib.dump(le, \"label_encoder.pkl\")\n",
    "print(\"Сохранено.\")\n",
    "\n",
    "# Оценка ROC-AUC\n",
    "y_prob = voting_ensemble.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(f\"ROC-AUC for Voting Ensemble (Weighted): {roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Ниже код оценки работы модели. На вход подается 1610 строк (все 805 grade_proof - 'подтверждён' и столько же grade_proof - 'не подтверждён)\n",
    "Датасет для проверки - balanced_data.json  \n",
    "Его также надо предварительно загрузить**\n",
    "\n",
    "\n",
    "\n",
    "На выходе получается информативный json со всеми промежуточными метриками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import joblib\n",
    "\n",
    "# Инициализация\n",
    "nltk.download('stopwords')\n",
    "russian_stopwords = set(stopwords.words('russian'))\n",
    "\n",
    "# Загрузка ансамбля обученных моделей и соотв. утилит\n",
    "voting_ensemble = joblib.load(\"voting_ensemble_model.pkl\")\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "label_encoder = joblib.load(\"label_encoder.pkl\")\n",
    "\n",
    "# Загрузка Transformer моделей\n",
    "model_work_similarity = SentenceTransformer('deepvk/USER-bge-m3')\n",
    "model_semantic = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Функции обработки данных\n",
    "def calculate_months(date_range):\n",
    "    try:\n",
    "        match = re.match(r'^(\\d{4}-\\d{2}-\\d{2})\\s*-\\s*(\\d{4}-\\d{2}-\\d{2})?$', date_range)\n",
    "        if not match:\n",
    "            return 0\n",
    "        start_date = datetime.strptime(match.group(1), '%Y-%m-%d')\n",
    "        end_date = datetime.strptime(match.group(2), '%Y-%m-%d') if match.group(2) else datetime.today()\n",
    "        return max((end_date.year - start_date.year) * 12 + (end_date.month - start_date.month), 0)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    words = text.split(',')\n",
    "    words = [re.sub(r'\\W+', '', word.strip()) for word in words]\n",
    "    words = [word for word in words if word and word not in russian_stopwords]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def calculate_skill_similarity(candidate_skills, work_experience):\n",
    "    candidate_skills_set = set(candidate_skills.split(\" \"))\n",
    "    work_experience_set = set(work_experience.split(\" \"))\n",
    "    all_skills = list(candidate_skills_set.union(work_experience_set))\n",
    "    candidate_vector = [1 if skill in candidate_skills_set else 0 for skill in all_skills]\n",
    "    experience_vector = [1 if skill in work_experience_set else 0 for skill in all_skills]\n",
    "    if not any(candidate_vector) or not any(experience_vector):\n",
    "        return 0.0\n",
    "    return cosine_similarity([candidate_vector], [experience_vector])[0][0]\n",
    "\n",
    "def calculate_semantic_similarity(position, key_skills, work_experience):\n",
    "    combined_skills = key_skills + \" \" + work_experience\n",
    "    position_embedding = model_semantic.encode([position])\n",
    "    skills_embedding = model_semantic.encode([combined_skills])\n",
    "    return cosine_similarity(position_embedding, skills_embedding)[0][0]\n",
    "\n",
    "def process_input_data(input_data):\n",
    "    results = []\n",
    "    for row in input_data:\n",
    "        work_experience = row.get(\"work_experience\", \"\")\n",
    "        position = preprocess_text(row.get(\"position\", \"\"))\n",
    "        key_skills = preprocess_text(row.get(\"key_skills\", \"\"))\n",
    "\n",
    "        total_months_worked = 0\n",
    "        processed_date_ranges = set()  # Для уникальности диапазонов дат\n",
    "\n",
    "        # Проход по строкам опыта работы\n",
    "        for line in work_experience.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Извлекаем диапазон дат и описание работы\n",
    "            date_part = line.split(':')[0].strip()\n",
    "            match = re.match(r'^(\\d{4}-\\d{2}-\\d{2})\\s*-\\s*(\\d{4}-\\d{2}-\\d{2})?$', date_part)\n",
    "            if match:\n",
    "                date_range = match.group(0)\n",
    "                details = line.split(':', 1)[-1].strip()\n",
    "                comparison_text = ' '.join(details.split()[:4])  # Используем первые 4 слова для сравнения\n",
    "\n",
    "                # Если диапазон уже обработан, пропускаем\n",
    "                if date_range in processed_date_ranges:\n",
    "                    continue\n",
    "                processed_date_ranges.add(date_range)\n",
    "\n",
    "                # Подсчитываем месяцы работы\n",
    "                months_worked = calculate_months(date_range)\n",
    "\n",
    "                # Вычисляем сходство между позицией и описанием работы\n",
    "                embeddings1 = model_work_similarity.encode(position, convert_to_tensor=True)\n",
    "                embeddings2 = model_work_similarity.encode(comparison_text, convert_to_tensor=True)\n",
    "                similarity = util.pytorch_cos_sim(embeddings1, embeddings2).item()\n",
    "\n",
    "                # Учитываем только релевантные месяцы работы\n",
    "                if similarity > 0.649:\n",
    "                    total_months_worked += months_worked\n",
    "\n",
    "       \n",
    "        skill_similarity = calculate_skill_similarity(key_skills, preprocess_text(work_experience))\n",
    "        semantic_similarity = calculate_semantic_similarity(position, key_skills, preprocess_text(work_experience))\n",
    "\n",
    "        # Добавляем обработанные данные\n",
    "        results.append({\n",
    "            \"age\": float(row.get(\"age\", 0)),\n",
    "            \"semantic_similarity\": semantic_similarity,\n",
    "            \"skill_similarity\": skill_similarity,\n",
    "            \"total_months_worked\": total_months_worked\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def main(filepath, output_filepath, num_records=1000):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Загрузка исходных данных\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        raw_data = json.load(f)\n",
    "\n",
    "    # Берем только первые num_records записей\n",
    "    raw_data = raw_data[:num_records]\n",
    "\n",
    "    # Сохраняем истинные метки\n",
    "    true_labels = [row[\"grade_proof\"] for row in raw_data]\n",
    "\n",
    "    # Обработка данных\n",
    "    processed_data = process_input_data(raw_data)\n",
    "\n",
    "    # Масштабирование\n",
    "    features = ['age', 'semantic_similarity', 'skill_similarity', 'total_months_worked']\n",
    "    scaled_data = pd.DataFrame(scaler.transform(processed_data[features]), columns=features)\n",
    "\n",
    "    # Предсказание\n",
    "    predictions_proba = voting_ensemble.predict_proba(scaled_data)[:, 1]\n",
    "    predictions = voting_ensemble.predict(scaled_data)\n",
    "\n",
    "    # Расчет метрики AUC-ROC\n",
    "    auc_roc = roc_auc_score(label_encoder.transform(true_labels), predictions_proba)\n",
    "\n",
    "    # Добавляем предсказания и промежуточные значения к данным\n",
    "    for row, prob, pred, true_label, intermediate in zip(raw_data, predictions_proba, predictions, true_labels, processed_data.to_dict(orient='records')):\n",
    "        row[\"predicted_grade_proof\"] = label_encoder.inverse_transform([pred])[0]\n",
    "        row[\"probability\"] = prob\n",
    "        row[\"grade_proof\"] = true_label  # Оригинальное значение\n",
    "\n",
    "        # Добавляем промежуточные значения\n",
    "        row[\"semantic_similarity\"] = intermediate[\"semantic_similarity\"]\n",
    "        row[\"skill_similarity\"] = intermediate[\"skill_similarity\"]\n",
    "        row[\"total_months_worked\"] = intermediate[\"total_months_worked\"]\n",
    "\n",
    "    # Сохранение результатов\n",
    "    with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(raw_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    avg_time_per_record = total_time / len(raw_data)\n",
    "\n",
    "    print(f\"ROC-AUC: {auc_roc}\")\n",
    "    print(f\"Results saved to {output_filepath}\")\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    print(f\"Number of records processed: {len(raw_data)}\")\n",
    "    print(f\"Average time per record: {avg_time_per_record:.4f} seconds\")\n",
    "\n",
    "\n",
    "# Выполнение\n",
    "input_filepath = \"/kaggle/input/balanced-data-flashteam-fic/balanced_data.json\"  # Путь к balanced_data.json\n",
    "output_filepath = \"output_results.json\"  \n",
    "main(input_filepath, output_filepath, num_records=1609) #Можно выбрать количество строк для проверки в num_records\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6204859,
     "sourceId": 10067585,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6204901,
     "sourceId": 10067640,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
